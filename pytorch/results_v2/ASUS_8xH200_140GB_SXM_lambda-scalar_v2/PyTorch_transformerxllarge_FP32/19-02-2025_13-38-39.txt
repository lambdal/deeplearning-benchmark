W0219 13:38:41.880000 3364 torch/distributed/run.py:793] 
W0219 13:38:41.880000 3364 torch/distributed/run.py:793] *****************************************
W0219 13:38:41.880000 3364 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0219 13:38:41.880000 3364 torch/distributed/run.py:793] *****************************************
6: thread affinity: {130, 382, 134, 138, 142, 146, 150, 154, 158, 162, 290, 166, 294, 170, 298, 174, 302, 178, 306, 182, 310, 186, 314, 190, 318, 322, 326, 330, 334, 338, 342, 346, 350, 98, 354, 102, 358, 106, 362, 110, 366, 378, 114, 370, 118, 374, 122, 126}
0: thread affinity: {0, 256, 4, 260, 8, 264, 12, 268, 16, 272, 20, 276, 24, 280, 28, 284, 32, 36, 40, 44, 48, 52, 56, 60, 64, 192, 68, 196, 72, 200, 76, 204, 80, 208, 84, 212, 88, 216, 92, 220, 224, 228, 232, 236, 240, 244, 248, 252}
5: thread affinity: {129, 133, 137, 141, 145, 149, 153, 157, 161, 289, 165, 293, 169, 297, 173, 301, 177, 305, 181, 309, 185, 313, 189, 317, 321, 325, 329, 333, 337, 341, 345, 349, 97, 353, 101, 357, 105, 361, 377, 109, 365, 113, 369, 381, 117, 373, 121, 125}
1: thread affinity: {1, 257, 5, 261, 9, 265, 13, 269, 17, 273, 21, 277, 25, 281, 29, 285, 33, 37, 41, 45, 49, 53, 57, 61, 65, 193, 69, 197, 73, 201, 77, 205, 81, 209, 85, 213, 89, 217, 93, 221, 225, 229, 233, 237, 241, 245, 249, 253}
4: thread affinity: {128, 132, 136, 140, 144, 148, 152, 156, 160, 288, 164, 292, 168, 296, 172, 300, 176, 304, 180, 308, 184, 312, 188, 316, 320, 324, 328, 332, 336, 340, 344, 380, 348, 96, 352, 100, 356, 376, 104, 360, 108, 364, 112, 368, 116, 372, 120, 124}
3: thread affinity: {3, 259, 7, 263, 11, 267, 15, 271, 19, 275, 23, 279, 27, 283, 31, 287, 35, 39, 43, 47, 51, 55, 59, 63, 67, 195, 71, 199, 75, 203, 79, 207, 83, 211, 87, 215, 91, 219, 95, 223, 227, 231, 235, 239, 243, 247, 251, 255}
2: thread affinity: {2, 258, 6, 262, 10, 266, 14, 270, 18, 274, 22, 278, 26, 282, 30, 286, 34, 38, 42, 46, 50, 54, 58, 62, 66, 194, 70, 198, 74, 202, 78, 206, 82, 210, 86, 214, 90, 218, 94, 222, 226, 230, 234, 238, 242, 246, 250, 254}
7: thread affinity: {131, 135, 383, 139, 143, 147, 151, 155, 159, 163, 291, 167, 295, 171, 299, 175, 303, 179, 307, 183, 311, 187, 315, 191, 319, 323, 327, 331, 335, 339, 343, 347, 351, 375, 99, 355, 103, 359, 107, 363, 111, 367, 115, 371, 379, 119, 123, 127}
[rank7]:[W219 13:39:15.651900520 ProcessGroupNCCL.cpp:4088] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W219 13:39:16.333809679 ProcessGroupNCCL.cpp:4088] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W219 13:39:16.360007715 ProcessGroupNCCL.cpp:4088] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W219 13:39:16.459785742 ProcessGroupNCCL.cpp:4088] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W219 13:39:16.486253477 ProcessGroupNCCL.cpp:4088] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W219 13:39:16.510159902 ProcessGroupNCCL.cpp:4088] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Experiment dir : LM-TFM
[rank0]:[W219 13:39:16.512099764 ProcessGroupNCCL.cpp:4088] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W219 13:39:16.512461408 ProcessGroupNCCL.cpp:4088] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Namespace(work_dir='LM-TFM', append_dataset=False, append_time=False, cuda=True, fp16=False, restart='', debug=False, log_all_ranks=False, dllog_file='train_log.json', txtlog_file='train_log.log', save_all=False, no_env=False, no_eval=True, no_test=False, log_interval=10, target_throughput=None, target_perplexity=None, apex_amp_opt_level='O2', amp='apex', affinity='socket_unique_interleaved', data='/data/transformer-xl/wikitext-103', dataset='wt103', vocab='word', n_layer=18, n_head=16, d_head=64, d_embed=1024, d_model=1024, d_inner=4096, dropout=0.2, dropatt=0.2, pre_lnorm=False, attn_type=0, not_tied=False, clamp_len=-1, adaptive=False, div_val=1, sample_softmax=-1, init='normal', emb_init='normal', init_range=0.1, emb_init_range=0.01, init_std=0.02, proj_init_std=0.01, optim='adam', lr=0.0, mom=0.0, scheduler='cosine', max_step_scheduler=None, warmup_step=16000, decay_rate=0.5, lr_min=0.0, clip=0.25, weight_decay=0.0, clip_nonemb=False, patience=0, eta_min=0.001, max_step=400, batch_size=384, local_batch_size=None, batch_chunk=1, roll=True, tgt_len=256, ext_len=0, mem_len=256, seed=1111, multi_gpu=None, gpu0_bsz=-1, same_length=False, varlen=False, swap_mem=False, eval_tgt_len=128, eval_batch_size=16, eval_max_steps=-1, eval_interval=5000, local_rank=0, tied=True)
world size: 8
Collecting environment information...
[rank1]: Traceback (most recent call last):
[rank1]:   File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/train.py", line 1134, in <module>
[rank1]:     main()
[rank1]:   File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/train.py", line 778, in main
[rank1]:     corpus = get_lm_corpus(args.data, args.dataset, args.vocab)
[rank1]:   File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/data_utils.py", line 322, in get_lm_corpus
[rank1]:     corpus = Corpus(datadir, dataset, vocab, **kwargs)
[rank1]:   File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/data_utils.py", line 247, in __init__
[rank1]:     self.vocab.count_file(os.path.join(path, 'train.txt'))
[rank1]:   File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/utils/vocabulary.py", line 58, in count_file
[rank1]:     assert os.path.exists(path)
[rank1]: AssertionError
[rank4]: Traceback (most recent call last):
[rank4]:   File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/train.py", line 1134, in <module>
[rank4]:     main()
[rank4]:   File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/train.py", line 778, in main
[rank4]:     corpus = get_lm_corpus(args.data, args.dataset, args.vocab)
[rank4]:   File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/data_utils.py", line 322, in get_lm_corpus
[rank4]:     corpus = Corpus(datadir, dataset, vocab, **kwargs)
[rank4]:   File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/data_utils.py", line 247, in __init__
[rank4]:     self.vocab.count_file(os.path.join(path, 'train.txt'))
[rank4]:   File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/utils/vocabulary.py", line 58, in count_file
[rank4]:     assert os.path.exists(path)
[rank4]: AssertionError
W0219 13:39:49.637000 3364 torch/distributed/elastic/multiprocessing/api.py:890] Sending process 3431 closing signal SIGTERM
W0219 13:39:49.638000 3364 torch/distributed/elastic/multiprocessing/api.py:890] Sending process 3433 closing signal SIGTERM
W0219 13:39:49.638000 3364 torch/distributed/elastic/multiprocessing/api.py:890] Sending process 3434 closing signal SIGTERM
W0219 13:39:49.638000 3364 torch/distributed/elastic/multiprocessing/api.py:890] Sending process 3435 closing signal SIGTERM
W0219 13:39:49.638000 3364 torch/distributed/elastic/multiprocessing/api.py:890] Sending process 3436 closing signal SIGTERM
W0219 13:39:49.638000 3364 torch/distributed/elastic/multiprocessing/api.py:890] Sending process 3437 closing signal SIGTERM
W0219 13:39:49.639000 3364 torch/distributed/elastic/multiprocessing/api.py:890] Sending process 3438 closing signal SIGTERM
E0219 13:39:51.520000 3364 torch/distributed/elastic/multiprocessing/api.py:862] failed (exitcode: 1) local_rank: 1 (pid: 3432) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.5.0a0+e000cf0ad9.nv24.10', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-19_13:39:49
  host      : 8fdc378caae0
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3432)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
DONE!
