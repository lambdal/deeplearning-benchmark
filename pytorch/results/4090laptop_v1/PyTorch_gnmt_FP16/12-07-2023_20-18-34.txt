0: thread affinity: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31}
0: Collecting environment information...
0: PyTorch version: 1.13.0a0+d0d6b1f
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.22.2
Libc version: glibc-2.31

Python version: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10)  [GCC 10.3.0] (64-bit runtime)
Python platform: Linux-5.19.0-46-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090 Laptop GPU
Nvidia driver version: 525.116.04
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.6.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] functorch==0.3.0a0
[pip3] numpy==1.22.2
[pip3] pytorch-quantization==2.1.2
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.13.0a0+d0d6b1f
[pip3] torch-tensorrt==1.3.0a0
[pip3] torchtext==0.11.0a0
[pip3] torchvision==0.14.0a0
[conda] functorch                 0.3.0a0                  pypi_0    pypi
[conda] mkl                       2020.4             h726a3e6_304    conda-forge
[conda] mkl-include               2020.4             h726a3e6_304    conda-forge
[conda] numpy                     1.22.2           py38h6ae9a64_0    conda-forge
[conda] pytorch-quantization      2.1.2                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.13.0a0+d0d6b1f          pypi_0    pypi
[conda] torch-tensorrt            1.3.0a0                  pypi_0    pypi
[conda] torchtext                 0.11.0a0                 pypi_0    pypi
[conda] torchvision               0.14.0a0                 pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=1, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=8, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 0.002
    maximize: False
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 383
0: Scheduler decay interval: 48
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 364522461
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][0/576]	Time 0.437 (0.000)	Data 2.87e-01 (0.00e+00)	Tok/s 26007 (0)	Loss/tok 10.5997 (10.5997)	LR 2.047e-05
0: TRAIN [0][10/576]	Time 0.229 (0.180)	Data 3.93e-05 (4.91e-05)	Tok/s 70708 (67945)	Loss/tok 9.6446 (10.0967)	LR 2.576e-05
0: TRAIN [0][20/576]	Time 0.170 (0.170)	Data 6.03e-05 (4.86e-05)	Tok/s 68906 (66799)	Loss/tok 9.2469 (9.7782)	LR 3.244e-05
0: TRAIN [0][30/576]	Time 0.116 (0.177)	Data 4.27e-05 (4.78e-05)	Tok/s 61188 (66864)	Loss/tok 8.7584 (9.5419)	LR 4.083e-05
0: TRAIN [0][40/576]	Time 0.228 (0.180)	Data 4.24e-05 (4.74e-05)	Tok/s 70269 (66772)	Loss/tok 8.7058 (9.3435)	LR 5.141e-05
0: TRAIN [0][50/576]	Time 0.122 (0.179)	Data 4.74e-05 (4.77e-05)	Tok/s 55709 (65609)	Loss/tok 8.2594 (9.1964)	LR 6.472e-05
0: TRAIN [0][60/576]	Time 0.237 (0.183)	Data 4.22e-05 (4.72e-05)	Tok/s 67451 (65394)	Loss/tok 8.3052 (9.0542)	LR 8.148e-05
0: TRAIN [0][70/576]	Time 0.235 (0.182)	Data 3.98e-05 (4.73e-05)	Tok/s 68875 (65020)	Loss/tok 8.1599 (8.9352)	LR 1.026e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][80/576]	Time 0.117 (0.184)	Data 4.65e-05 (4.72e-05)	Tok/s 60014 (65397)	Loss/tok 7.7211 (8.8337)	LR 1.291e-04
0: TRAIN [0][90/576]	Time 0.235 (0.183)	Data 4.39e-05 (4.74e-05)	Tok/s 68646 (65361)	Loss/tok 8.0118 (8.7393)	LR 1.626e-04
0: TRAIN [0][100/576]	Time 0.230 (0.180)	Data 4.29e-05 (4.72e-05)	Tok/s 69931 (65140)	Loss/tok 7.8976 (8.6614)	LR 2.047e-04
0: TRAIN [0][110/576]	Time 0.228 (0.182)	Data 4.24e-05 (4.71e-05)	Tok/s 70403 (65371)	Loss/tok 7.7942 (8.5759)	LR 2.576e-04
0: TRAIN [0][120/576]	Time 0.116 (0.179)	Data 4.60e-05 (4.71e-05)	Tok/s 59664 (65224)	Loss/tok 7.3898 (8.5176)	LR 3.244e-04
0: TRAIN [0][130/576]	Time 0.168 (0.179)	Data 4.60e-05 (4.71e-05)	Tok/s 69316 (65324)	Loss/tok 7.5908 (8.4546)	LR 4.083e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][140/576]	Time 0.171 (0.180)	Data 4.39e-05 (4.70e-05)	Tok/s 67425 (65532)	Loss/tok 7.6446 (8.3991)	LR 5.141e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][150/576]	Time 0.116 (0.179)	Data 4.67e-05 (4.69e-05)	Tok/s 59868 (65597)	Loss/tok 7.3743 (8.3560)	LR 6.472e-04
0: TRAIN [0][160/576]	Time 0.169 (0.181)	Data 4.65e-05 (4.70e-05)	Tok/s 67989 (65671)	Loss/tok 7.5862 (8.3169)	LR 8.148e-04
0: TRAIN [0][170/576]	Time 0.116 (0.181)	Data 4.63e-05 (4.70e-05)	Tok/s 58984 (65621)	Loss/tok 7.2839 (8.2840)	LR 1.026e-03
0: TRAIN [0][180/576]	Time 0.302 (0.182)	Data 4.63e-05 (4.70e-05)	Tok/s 69237 (65753)	Loss/tok 7.8214 (8.2448)	LR 1.291e-03
0: TRAIN [0][190/576]	Time 0.176 (0.182)	Data 5.15e-05 (4.69e-05)	Tok/s 65748 (65650)	Loss/tok 7.6218 (8.2152)	LR 1.626e-03
0: TRAIN [0][200/576]	Time 0.309 (0.182)	Data 4.63e-05 (4.68e-05)	Tok/s 67587 (65542)	Loss/tok 7.6971 (8.1829)	LR 2.000e-03
0: TRAIN [0][210/576]	Time 0.171 (0.182)	Data 4.36e-05 (4.67e-05)	Tok/s 67460 (65489)	Loss/tok 7.2428 (8.1477)	LR 2.000e-03
0: TRAIN [0][220/576]	Time 0.229 (0.182)	Data 4.58e-05 (4.69e-05)	Tok/s 70531 (65542)	Loss/tok 7.3791 (8.1083)	LR 2.000e-03
0: TRAIN [0][230/576]	Time 0.173 (0.182)	Data 4.43e-05 (4.69e-05)	Tok/s 66820 (65596)	Loss/tok 7.0348 (8.0661)	LR 2.000e-03
0: TRAIN [0][240/576]	Time 0.235 (0.182)	Data 4.55e-05 (4.69e-05)	Tok/s 68819 (65651)	Loss/tok 7.0114 (8.0217)	LR 2.000e-03
0: TRAIN [0][250/576]	Time 0.230 (0.184)	Data 4.41e-05 (4.68e-05)	Tok/s 69785 (65783)	Loss/tok 6.9502 (7.9706)	LR 2.000e-03
0: TRAIN [0][260/576]	Time 0.171 (0.183)	Data 4.48e-05 (4.68e-05)	Tok/s 66979 (65743)	Loss/tok 6.7533 (7.9325)	LR 2.000e-03
0: TRAIN [0][270/576]	Time 0.123 (0.184)	Data 4.32e-05 (4.68e-05)	Tok/s 57270 (65741)	Loss/tok 6.3597 (7.8853)	LR 2.000e-03
0: TRAIN [0][280/576]	Time 0.303 (0.184)	Data 4.48e-05 (4.68e-05)	Tok/s 68330 (65593)	Loss/tok 6.9246 (7.8485)	LR 2.000e-03
0: TRAIN [0][290/576]	Time 0.121 (0.184)	Data 5.03e-05 (4.67e-05)	Tok/s 56607 (65455)	Loss/tok 6.3140 (7.8124)	LR 2.000e-03
0: TRAIN [0][300/576]	Time 0.179 (0.185)	Data 4.55e-05 (4.68e-05)	Tok/s 64481 (65433)	Loss/tok 6.3931 (7.7674)	LR 2.000e-03
0: TRAIN [0][310/576]	Time 0.180 (0.184)	Data 4.96e-05 (4.68e-05)	Tok/s 63936 (65247)	Loss/tok 6.3653 (7.7323)	LR 2.000e-03
0: TRAIN [0][320/576]	Time 0.120 (0.183)	Data 4.46e-05 (4.68e-05)	Tok/s 57818 (65022)	Loss/tok 6.0976 (7.7008)	LR 2.000e-03
0: TRAIN [0][330/576]	Time 0.180 (0.183)	Data 4.89e-05 (4.68e-05)	Tok/s 64231 (64942)	Loss/tok 6.3232 (7.6619)	LR 2.000e-03
0: TRAIN [0][340/576]	Time 0.179 (0.183)	Data 4.32e-05 (4.69e-05)	Tok/s 64461 (64824)	Loss/tok 6.2295 (7.6260)	LR 2.000e-03
0: TRAIN [0][350/576]	Time 0.177 (0.184)	Data 4.91e-05 (4.69e-05)	Tok/s 65814 (64818)	Loss/tok 6.1078 (7.5817)	LR 2.000e-03
0: TRAIN [0][360/576]	Time 0.176 (0.186)	Data 5.08e-05 (4.69e-05)	Tok/s 65117 (64813)	Loss/tok 6.1133 (7.5380)	LR 2.000e-03
0: TRAIN [0][370/576]	Time 0.307 (0.185)	Data 4.43e-05 (4.69e-05)	Tok/s 68193 (64674)	Loss/tok 6.3396 (7.5054)	LR 2.000e-03
0: TRAIN [0][380/576]	Time 0.241 (0.185)	Data 4.53e-05 (4.68e-05)	Tok/s 67164 (64626)	Loss/tok 6.0917 (7.4656)	LR 2.000e-03
0: TRAIN [0][390/576]	Time 0.178 (0.185)	Data 5.32e-05 (4.68e-05)	Tok/s 64804 (64625)	Loss/tok 5.8252 (7.4272)	LR 1.000e-03
0: TRAIN [0][400/576]	Time 0.123 (0.186)	Data 4.36e-05 (4.69e-05)	Tok/s 57325 (64652)	Loss/tok 5.4757 (7.3842)	LR 1.000e-03
0: TRAIN [0][410/576]	Time 0.123 (0.187)	Data 4.55e-05 (4.69e-05)	Tok/s 57560 (64650)	Loss/tok 5.4172 (7.3431)	LR 1.000e-03
0: TRAIN [0][420/576]	Time 0.313 (0.187)	Data 4.79e-05 (4.69e-05)	Tok/s 66751 (64627)	Loss/tok 6.0168 (7.3036)	LR 1.000e-03
0: TRAIN [0][430/576]	Time 0.177 (0.188)	Data 5.13e-05 (4.69e-05)	Tok/s 65073 (64600)	Loss/tok 5.5800 (7.2671)	LR 5.000e-04
0: TRAIN [0][440/576]	Time 0.242 (0.187)	Data 4.46e-05 (4.68e-05)	Tok/s 66892 (64553)	Loss/tok 5.8014 (7.2334)	LR 5.000e-04
0: TRAIN [0][450/576]	Time 0.239 (0.187)	Data 4.58e-05 (4.69e-05)	Tok/s 67596 (64571)	Loss/tok 5.7195 (7.1965)	LR 5.000e-04
0: TRAIN [0][460/576]	Time 0.122 (0.187)	Data 4.63e-05 (4.70e-05)	Tok/s 56736 (64522)	Loss/tok 5.1162 (7.1633)	LR 5.000e-04
0: TRAIN [0][470/576]	Time 0.242 (0.188)	Data 4.79e-05 (4.69e-05)	Tok/s 67137 (64495)	Loss/tok 5.6773 (7.1284)	LR 5.000e-04
0: TRAIN [0][480/576]	Time 0.122 (0.188)	Data 4.82e-05 (4.70e-05)	Tok/s 55709 (64458)	Loss/tok 5.1072 (7.0957)	LR 2.500e-04
0: TRAIN [0][490/576]	Time 0.178 (0.188)	Data 4.94e-05 (4.69e-05)	Tok/s 64376 (64376)	Loss/tok 5.3903 (7.0656)	LR 2.500e-04
0: TRAIN [0][500/576]	Time 0.239 (0.188)	Data 4.65e-05 (4.69e-05)	Tok/s 67561 (64356)	Loss/tok 5.6329 (7.0331)	LR 2.500e-04
0: TRAIN [0][510/576]	Time 0.123 (0.188)	Data 4.53e-05 (4.69e-05)	Tok/s 54829 (64284)	Loss/tok 5.0094 (7.0068)	LR 2.500e-04
0: TRAIN [0][520/576]	Time 0.123 (0.187)	Data 4.89e-05 (4.68e-05)	Tok/s 57699 (64221)	Loss/tok 5.0379 (6.9814)	LR 2.500e-04
0: TRAIN [0][530/576]	Time 0.179 (0.188)	Data 6.79e-05 (4.69e-05)	Tok/s 64588 (64205)	Loss/tok 5.3401 (6.9508)	LR 1.250e-04
0: TRAIN [0][540/576]	Time 0.178 (0.188)	Data 4.51e-05 (4.69e-05)	Tok/s 64565 (64232)	Loss/tok 5.3507 (6.9191)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][550/576]	Time 0.241 (0.188)	Data 4.51e-05 (4.69e-05)	Tok/s 67089 (64217)	Loss/tok 5.5323 (6.8918)	LR 1.250e-04
0: TRAIN [0][560/576]	Time 0.176 (0.189)	Data 6.89e-04 (4.81e-05)	Tok/s 65612 (64250)	Loss/tok 5.2985 (6.8624)	LR 1.250e-04
0: TRAIN [0][570/576]	Time 0.239 (0.189)	Data 2.60e-05 (4.78e-05)	Tok/s 67155 (64220)	Loss/tok 5.5196 (6.8376)	LR 1.250e-04
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [0][0/80]	Time 0.075 (0.000)	Data 9.95e-04 (0.00e+00)	Tok/s 139040 (0)	Loss/tok 6.7337 (6.7337)
0: VALIDATION [0][10/80]	Time 0.027 (0.034)	Data 7.90e-04 (8.19e-04)	Tok/s 219752 (205244)	Loss/tok 6.4669 (6.5541)
0: VALIDATION [0][20/80]	Time 0.021 (0.029)	Data 7.09e-04 (7.91e-04)	Tok/s 220883 (209601)	Loss/tok 6.1983 (6.4741)
0: VALIDATION [0][30/80]	Time 0.018 (0.026)	Data 7.02e-04 (7.72e-04)	Tok/s 217047 (211794)	Loss/tok 6.0610 (6.4064)
0: VALIDATION [0][40/80]	Time 0.016 (0.023)	Data 7.13e-04 (7.58e-04)	Tok/s 207856 (211141)	Loss/tok 6.1352 (6.3675)
0: VALIDATION [0][50/80]	Time 0.014 (0.022)	Data 7.25e-04 (7.52e-04)	Tok/s 186038 (210107)	Loss/tok 5.8788 (6.3341)
0: VALIDATION [0][60/80]	Time 0.010 (0.020)	Data 7.30e-04 (7.45e-04)	Tok/s 205894 (209921)	Loss/tok 6.0993 (6.3072)
0: VALIDATION [0][70/80]	Time 0.008 (0.018)	Data 7.04e-04 (7.40e-04)	Tok/s 207593 (209467)	Loss/tok 5.7344 (6.2762)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [0][9/24]	Time 0.3200 (0.5669)	Decoder iters 149.0 (149.0)	Tok/s 26486 (24925)
0: TEST [0][19/24]	Time 0.1917 (0.4060)	Decoder iters 97.0 (143.4)	Tok/s 22703 (25080)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 0	Training Loss: 6.8232	Validation Loss: 6.2475	Test BLEU: 1.41
0: Performance: Epoch: 0	Training: 64241 Tok/s	Validation: 206413 Tok/s
0: Finished epoch 0
0: Total training time 129 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       1|                 256|                      1.41|            64241.41689917352|             2.149888809521993|
DONE!
